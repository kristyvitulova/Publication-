#weakly supervised version
import os
import torch
import numpy as np
import matplotlib.pyplot as plt
from torch import nn, optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from scipy.signal import stft

# --------------------------
# Spectrogram Utils
# --------------------------
def compute_spectrogram(waveform, nperseg=512, noverlap=256):
    f, t, Zxx = stft(waveform, nperseg=nperseg, noverlap=noverlap, fs=4096)
    spec = np.abs(Zxx)
    if spec.shape[1] > 4:
        spec = spec[:, 1:-1]
    return spec

def preprocess_waveforms_to_spectrograms(waveforms):
    processed = []
    for waveform in waveforms:
        spectrogram = compute_spectrogram(waveform.numpy())
        min_val, max_val = np.min(spectrogram), np.max(spectrogram)
        if max_val > min_val:
            spectrogram = (spectrogram - min_val) / (max_val - min_val)
        else:
            spectrogram = np.zeros_like(spectrogram)
        processed.append(spectrogram)
    return torch.tensor(processed, dtype=torch.float32)

def load_all_from_folder(folder_path):
    all_waveforms = []
    for filename in os.listdir(folder_path):
        if filename.endswith('.pt'):
            data = torch.load(os.path.join(folder_path, filename))
            waveforms = data.get('waveforms', data) if isinstance(data, dict) else data
            all_waveforms.append(waveforms)
    return torch.cat(all_waveforms)

# --------------------------
# Model Definition (Trial 13)
# --------------------------
class DeeperAutoencoderWithRegularization(nn.Module):
    def __init__(self):
        super().__init__()
        d = 0.08028083975596854
        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.drop1 = nn.Dropout(d)
        self.pool1 = nn.MaxPool2d(2, stride=2, return_indices=True)

        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.drop2 = nn.Dropout(d)
        self.pool2 = nn.MaxPool2d(2, stride=2, return_indices=True)

        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.drop3 = nn.Dropout(d)
        self.pool3 = nn.MaxPool2d(2, stride=2, return_indices=True)

        self.unpool1 = nn.MaxUnpool2d(2, stride=2)
        self.deconv1 = nn.ConvTranspose2d(128, 64, 3, padding=1)
        self.bn4 = nn.BatchNorm2d(64)
        self.drop4 = nn.Dropout(d)

        self.unpool2 = nn.MaxUnpool2d(2, stride=2)
        self.deconv2 = nn.ConvTranspose2d(64, 32, 3, padding=1)
        self.bn5 = nn.BatchNorm2d(32)
        self.drop5 = nn.Dropout(d)

        self.unpool3 = nn.MaxUnpool2d(2, stride=2)
        self.deconv3 = nn.ConvTranspose2d(32, 1, 3, padding=1)

    def forward(self, x):
        orig = x.size()
        x = self.conv1(x); x = self.bn1(x); x = self.drop1(x); x, i1 = self.pool1(x); s1 = x.size()
        x = self.conv2(x); x = self.bn2(x); x = self.drop2(x); x, i2 = self.pool2(x); s2 = x.size()
        x = self.conv3(x); x = self.bn3(x); x = self.drop3(x); x, i3 = self.pool3(x); s3 = x.size()
        x = self.unpool1(x, i3, output_size=s2); x = self.deconv1(x); x = self.bn4(x); x = self.drop4(x)
        x = self.unpool2(x, i2, output_size=s1); x = self.deconv2(x); x = self.bn5(x); x = self.drop5(x)
        x = self.unpool3(x, i1, output_size=orig); x = self.deconv3(x)
        return x

# --------------------------
# Load and Prepare Data
# --------------------------
noise_folder = "/home/kristyna/Desktop/ET_segments_waveforms_final_cutoff0"
raw_waveforms = load_all_from_folder(noise_folder)
spectrograms = preprocess_waveforms_to_spectrograms(raw_waveforms)
spectrograms = torch.nn.functional.interpolate(spectrograms.unsqueeze(1), size=(256, 31))

train_data, temp_data = train_test_split(spectrograms, test_size=0.3, random_state=42)
val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)

train_loader = DataLoader(TensorDataset(train_data), batch_size=16, shuffle=True)
val_loader = DataLoader(TensorDataset(val_data), batch_size=16, shuffle=False)
test_loader = DataLoader(TensorDataset(test_data), batch_size=32, shuffle=False)

# --------------------------
# Training Setup (Trial 13)
# --------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = DeeperAutoencoderWithRegularization().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(
    model.parameters(),
    lr=0.0005950527652410784,
    weight_decay=7.216242101650261e-07
)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.1, patience=3, verbose=True
)

# --------------------------
# Training Loop
# --------------------------
num_epochs = 70
train_losses, val_losses = [], []
train_errors_all, val_errors_all = [], []

for epoch in range(num_epochs):
    model.train()
    epoch_train_loss, epoch_train_errors = 0.0, []

    for batch in train_loader:
        x = batch[0].to(device)
        optimizer.zero_grad()
        y = model(x)
        loss = criterion(y, x)
        loss.backward()
        optimizer.step()
        epoch_train_loss += loss.item()
        epoch_train_errors.extend(((y - x)**2).mean(dim=[1, 2, 3]).detach().cpu().numpy())

    train_losses.append(epoch_train_loss / len(train_loader))
    train_errors_all.extend(epoch_train_errors)

    model.eval()
    epoch_val_loss, epoch_val_errors = 0.0, []

    with torch.no_grad():
        for batch in val_loader:
            x = batch[0].to(device)
            y = model(x)
            loss = criterion(y, x)
            epoch_val_loss += loss.item()
            epoch_val_errors.extend(((y - x)**2).mean(dim=[1, 2, 3]).cpu().numpy())

    val_losses.append(epoch_val_loss / len(val_loader))
    val_errors_all.extend(epoch_val_errors)
    scheduler.step(epoch_val_loss)

    print(f"[Epoch {epoch+1}/{num_epochs}] Train Loss: {train_losses[-1]:.6f}, Val Loss: {val_losses[-1]:.6f}")

# --------------------------
# Save Model
# --------------------------
model_path = "/home/kristyna/Desktop/autoencoder_ET_0Hz_param2_paper.pth"
torch.save(model.state_dict(), model_path)
print(f"Model saved to {model_path}")

# --------------------------
# Plot Losses
# --------------------------
plt.figure(figsize=(10, 5))
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training and Validation Loss")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("train_val_loss_curve_trial13.pdf")
plt.show()

# --------------------------
# Evaluation: Test Set
# --------------------------
noise_only_errors = []
model.eval()
with torch.no_grad():
    for batch in test_loader:
        x = batch[0].to(device)
        y = model(x)
        batch_errors = ((y - x) ** 2).mean(dim=[1, 2, 3])
        noise_only_errors.extend(batch_errors.cpu().numpy())

threshold = np.mean(noise_only_errors) + 2 * np.std(noise_only_errors)
threshold3 = np.mean(noise_only_errors) + 3 * np.std(noise_only_errors)
print(f"Anomaly detection threshold: {threshold:.6f}")
print(f"Anomaly detection threshold3: {threshold3:.6f}")

# --------------------------
# Plot: Histogram
# --------------------------
plt.figure(figsize=(10, 6))
plt.hist(noise_only_errors, bins=50, alpha=0.7, label='Noise-Only Errors', color='cornflowerblue')
plt.hist(signal_errors, bins=50, alpha=0.7, label='Signal+Noise Errors', color='orange')
plt.axvline(threshold, color='red', linestyle='--', label='Anomaly Threshold')
plt.xlabel("Reconstruction Error")
plt.ylabel("Frequency")
plt.title("Distribution of Reconstruction Errors")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig("comparison_histogram_noise_vs_signal.pdf")
plt.show()
